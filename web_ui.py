import streamlit as st
import requests
import json

st.set_page_config(page_title="GuguGaga Chat", page_icon="ğŸ¤–")
st.title("ğŸ¤– GuguGaga AI Assistant")

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.header("å‚æ•°è®¾ç½®")
    temperature = st.slider("Temperature", 0.0, 1.0, 0.85)
    top_p = st.slider("Top P", 0.0, 1.0, 0.85)
    max_tokens = st.number_input("Max New Tokens", 128, 8192, 2048)
    
    if st.button("æ¸…é™¤å¯¹è¯å†å²"):
        st.session_state.messages = []
        st.rerun()

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è¯´ç‚¹ä»€ä¹ˆ..."):
    
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            # è°ƒç”¨ API
            api_url = "http://localhost:8000/chat"
            payload = {
                "messages": st.session_state.messages,
                "temperature": temperature,
                "top_p": top_p,
                "max_new_tokens": max_tokens
            }
            
            with requests.post(api_url, json=payload, stream=True) as response:
                if response.status_code == 200:
                    for chunk in response.iter_content(chunk_size=None, decode_unicode=True):
                        if chunk:
                            full_response += chunk
                            message_placeholder.markdown(full_response + "â–Œ")
                    message_placeholder.markdown(full_response)
                else:
                    st.error(f"Error: {response.status_code} - {response.text}")
                    
        except Exception as e:
            st.error(f"è¿æ¥åç«¯å¤±è´¥: {e}")

    if full_response:
        st.session_state.messages.append({"role": "assistant", "content": full_response})